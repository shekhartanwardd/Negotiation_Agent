{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Escalation Model ML Pipeline\n",
        "\n",
        "This notebook provides a complete feature engineering and prediction pipeline for the escalation model.\n",
        "\n",
        "## Pipeline Overview:\n",
        "1. **Load Dataset**: Load the raw dataset from CSV\n",
        "2. **Feature Engineering**: Create derived features (IS_ND, IS_MnI, IS_PFQ, IS_OSI, etc.)\n",
        "3. **Data Preprocessing**: Handle missing values, type conversions, and log transformations\n",
        "4. **Model Loading**: Load the pre-trained LightGBM model\n",
        "5. **Inference**: Generate escalation predictions\n",
        "6. **Evaluation**: Compute performance metrics (if labels available)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All packages imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Standard library\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.metrics import (\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score,\n",
        "    roc_auc_score,\n",
        "    auc,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Utilities\n",
        "from functools import reduce\n",
        "import operator\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "print(\"All packages imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total features: 70\n",
            "Categorical features: 10\n",
            "Numeric features: 60\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Modify these paths as needed\n",
        "# ============================================================\n",
        "\n",
        "# Dataset path\n",
        "DATASET_PATH = '/Users/shekhar.tanwar/Documents/Projects/NegotiatonAgent/dataset/processed_dataset/dataset.csv'\n",
        "\n",
        "# Model path\n",
        "MODEL_PATH = '/Users/shekhar.tanwar/Documents/Projects/NegotiatonAgent/model/v1_lgb_1125_fold3.pkl'\n",
        "\n",
        "# Target column\n",
        "TARGET_COLUMN = 'IS_ESCALATED'\n",
        "\n",
        "# ============================================================\n",
        "# Features from (Clone) 3-v2-training.ipynb - top30_features (actually 70 features)\n",
        "# ============================================================\n",
        "\n",
        "FINAL_FEATURES = [\n",
        "    'SH_CNR', 'SH_IS_CREDITS', 'DEFECT_CATEGORY', 'SH_IS_REFUND', 'SH_IS_REDELIVERY',\n",
        "    'IS_MnI', 'SH_FIRST_REPORT_ISSUE', 'SH_LATEST_REPORT_ISSUE', 'MTO_ORDER_COUNT_L90D',\n",
        "    'FRAUD_CNR_REQUEST_RATIO_L60D', 'MTO_ORDER_COUNT_L12M', 'SH_IS_REJET',\n",
        "    'FRAUD_CNR_APPROVED_REQUESTS_COUNT_L60D', 'AVG_VP_LIFETIME', 'IS_OSI',\n",
        "    'MTO_ORDER_COUNT_L28D', 'FRAUD_CNR_AMOUNT_L60D', 'ACTUAL_VP_RAW_AMT_L12M',\n",
        "    'MTO_ORDER_COUNT_LIFETIME', 'CREDIT_REFUND_ORDER_COUNT_L90D', 'IS_ND',\n",
        "    'IS_20_MIN_LATE', 'MOST_FREQ_MTO_COUNT', 'FRAUD_CNR_REQUEST_RATIO_L180D',\n",
        "    'NEVER_DELIVERED_COUNT_L90D', 'DEFECT_DELIVERY_COUNT_L12M',\n",
        "    'HIGH_QUALITY_DELIVERY_COUNT_L12M', 'NEVER_DELIVERED_COUNT_L12M',\n",
        "    'ORDER_COUNT_L12M', 'IS_PFQ', 'ACTUAL_DELIVERIES_COUNT_L12M',\n",
        "    'FRAUD_CNR_AMOUNT_L180D', 'AVG_VP_LIFETIME_CATEGORY', 'ORDER_COUNT_L90D',\n",
        "    'MOST_FREQ_MTO_ISSUE', 'FRAUD_CNR_REQUESTED_DELIVERIES_COUNT_L180D',\n",
        "    'MTO_ORDER_COUNT_L7D', 'FRAUD_CNR_APPROVED_REQUESTS_COUNT_L180D',\n",
        "    'CREDIT_REFUND_ORDER_COUNT_L12M', 'ORDER_COUNT_LIFETIME',\n",
        "    'ML_CX_CNR_RISK_V1_SCORE', 'LATEST_MTO_ISSUE', 'SUBTOTAL',\n",
        "    'HIGH_QUALITY_DELIVERY_COUNT_L90D', 'NEVER_DELIVERED_COUNT_L28D',\n",
        "    'TOTAL_ITEM_COUNT', 'IS_TOP_95_PERCENT_VP', 'FRAUD_CNR_ND_ORDERS_COUNT_L60D',\n",
        "    'NEVER_DELIVERED_COUNT_LIFETIME', 'ORDER_COUNT_L28D', 'PROMOTIONS',\n",
        "    'AVG_SPEND_LIFETIME', 'DEFECT_DELIVERY_COUNT_L90D', 'DEFAULT_ZIP_CODE',\n",
        "    'CREDIT_REFUND_ORDER_COUNT_L28D', 'AVG_GOV_LIFETIME',\n",
        "    'CREDIT_REFUND_ORDER_COUNT_LIFETIME', 'TOTAL_MAIN_VISITOR_COUNT_L90D',\n",
        "    'NEVER_DELIVERED_COUNT_L7D', 'FRAUD_CNR_ISSUANCE_AMOUNT_LIFETIME',\n",
        "    'DEFECT_DELIVERY_COUNT_LIFETIME', 'HIGH_QUALITY_DELIVERY_COUNT_L28D',\n",
        "    'FRAUD_CNR_GOV_AMOUNT_LIFETIME', 'SUBMIT_PLATFORM', 'AVG_SPEND_LIFETIME_CATEGORY',\n",
        "    'IS_ELITE_CX', 'HOMEPAGE_SESSION_COUNT_L90D', 'EARLY_MORNING_COUNT_RATIO_LIFETIME',\n",
        "    'CANCEL_COUNT_LIFETIME', 'HIGH_QUALITY_DELIVERY_COUNT_LIFETIME'\n",
        "]\n",
        "\n",
        "# Categorical features (intersection of top30_features and cat_features from training)\n",
        "# These are the categorical features from the training that are present in FINAL_FEATURES\n",
        "CATEGORICAL_FEATURES = [\n",
        "    'DEFECT_CATEGORY', 'SH_FIRST_REPORT_ISSUE', 'SH_LATEST_REPORT_ISSUE',\n",
        "    'DEFAULT_ZIP_CODE', 'IS_TOP_95_PERCENT_VP', 'AVG_SPEND_LIFETIME_CATEGORY',\n",
        "    'AVG_VP_LIFETIME_CATEGORY', 'MOST_FREQ_MTO_ISSUE', 'LATEST_MTO_ISSUE', 'SUBMIT_PLATFORM'\n",
        "]\n",
        "\n",
        "# Numeric features (all FINAL_FEATURES except CATEGORICAL_FEATURES)\n",
        "NUMERIC_FEATURES = [f for f in FINAL_FEATURES if f not in CATEGORICAL_FEATURES]\n",
        "\n",
        "print(f\"Total features: {len(FINAL_FEATURES)}\")\n",
        "print(f\"Categorical features: {len(CATEGORICAL_FEATURES)}\")\n",
        "print(f\"Numeric features: {len(NUMERIC_FEATURES)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from: /Users/shekhar.tanwar/Documents/Projects/NegotiatonAgent/dataset/processed_dataset/dataset.csv\n",
            "Dataset shape: (1414041, 195)\n",
            "Columns: 195\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>DELIVERY_ID</th>\n",
              "      <th>CX_ID</th>\n",
              "      <th>STORE_ID</th>\n",
              "      <th>DEFECT_DATE</th>\n",
              "      <th>DEFECT_CATEGORY</th>\n",
              "      <th>DEFECT_TIMESTAMP_UTC</th>\n",
              "      <th>IS_DP_CX</th>\n",
              "      <th>IS_ELITE_CX</th>\n",
              "      <th>IS_WHALE_CX</th>\n",
              "      <th>...</th>\n",
              "      <th>IS_ESCALATED</th>\n",
              "      <th>ESCALATION_TIME</th>\n",
              "      <th>AGENT_ISSUED_AC</th>\n",
              "      <th>CHATBOT_ISSUED_AC</th>\n",
              "      <th>SH_ISSUED_AC</th>\n",
              "      <th>CONVERSATION</th>\n",
              "      <th>AGENT_ISSUED_AC_FLAG</th>\n",
              "      <th>ACTUAL_AC_CONVERSATION</th>\n",
              "      <th>Parsed_AC</th>\n",
              "      <th>CONVERSATION_HUMAN_AGENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2.179857e+11</td>\n",
              "      <td>1949223925</td>\n",
              "      <td>29736287</td>\n",
              "      <td>2025-09-05</td>\n",
              "      <td>Unknown or Unspecified Issue</td>\n",
              "      <td>2025-09-05 18:10:40.536000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-09-05 18:14:48.000000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>Chatbot: Hi Deepak, I'm your DoorDash virtual ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>3.560231e+11</td>\n",
              "      <td>1884080059</td>\n",
              "      <td>1130627</td>\n",
              "      <td>2025-09-03</td>\n",
              "      <td>Unknown or Unspecified Issue</td>\n",
              "      <td>2025-09-03 17:18:12.930000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-09-03 19:59:40.000000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>Chatbot: Hi aj, I'm your DoorDash virtual assi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>2.621826e+11</td>\n",
              "      <td>76381204</td>\n",
              "      <td>1130627</td>\n",
              "      <td>2025-09-03</td>\n",
              "      <td>Missing or Incorrect Items</td>\n",
              "      <td>2025-09-03 23:13:03.000000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-09-03 23:13:03.000000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>Chatbot: Hi Luke, I'm your DoorDash virtual as...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12</td>\n",
              "      <td>2.768723e+11</td>\n",
              "      <td>258188252</td>\n",
              "      <td>1130627</td>\n",
              "      <td>2025-09-03</td>\n",
              "      <td>Unknown or Unspecified Issue</td>\n",
              "      <td>2025-09-04 01:42:29.205000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-09-04 01:43:06.000000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>Chatbot: Hi Colin, I'm your DoorDash virtual a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>3.184242e+11</td>\n",
              "      <td>1965318695</td>\n",
              "      <td>281439</td>\n",
              "      <td>2025-09-06</td>\n",
              "      <td>Unknown or Unspecified Issue</td>\n",
              "      <td>2025-09-06 17:12:46.960000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-09-07 14:43:45.988205000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>Chatbot: Hi Cassandra, I'm your DoorDash virtu...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 195 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0   DELIVERY_ID       CX_ID  STORE_ID DEFECT_DATE  \\\n",
              "0           0  2.179857e+11  1949223925  29736287  2025-09-05   \n",
              "1           2  3.560231e+11  1884080059   1130627  2025-09-03   \n",
              "2          11  2.621826e+11    76381204   1130627  2025-09-03   \n",
              "3          12  2.768723e+11   258188252   1130627  2025-09-03   \n",
              "4          43  3.184242e+11  1965318695    281439  2025-09-06   \n",
              "\n",
              "                DEFECT_CATEGORY           DEFECT_TIMESTAMP_UTC  IS_DP_CX  \\\n",
              "0  Unknown or Unspecified Issue  2025-09-05 18:10:40.536000000         1   \n",
              "1  Unknown or Unspecified Issue  2025-09-03 17:18:12.930000000         0   \n",
              "2    Missing or Incorrect Items  2025-09-03 23:13:03.000000000         1   \n",
              "3  Unknown or Unspecified Issue  2025-09-04 01:42:29.205000000         0   \n",
              "4  Unknown or Unspecified Issue  2025-09-06 17:12:46.960000000         1   \n",
              "\n",
              "   IS_ELITE_CX  IS_WHALE_CX  ...  IS_ESCALATED                ESCALATION_TIME  \\\n",
              "0            0            0  ...             1  2025-09-05 18:14:48.000000000   \n",
              "1            0            0  ...             1  2025-09-03 19:59:40.000000000   \n",
              "2            0            0  ...             1  2025-09-03 23:13:03.000000000   \n",
              "3            0            0  ...             1  2025-09-04 01:43:06.000000000   \n",
              "4            0            0  ...             1  2025-09-07 14:43:45.988205000   \n",
              "\n",
              "   AGENT_ISSUED_AC CHATBOT_ISSUED_AC SH_ISSUED_AC  \\\n",
              "0             -1.0              -1.0         -1.0   \n",
              "1             -1.0              -1.0         -1.0   \n",
              "2             -1.0              -1.0         -1.0   \n",
              "3             -1.0              -1.0         -1.0   \n",
              "4             -1.0              -1.0         -1.0   \n",
              "\n",
              "                                        CONVERSATION  AGENT_ISSUED_AC_FLAG  \\\n",
              "0  Chatbot: Hi Deepak, I'm your DoorDash virtual ...                     0   \n",
              "1  Chatbot: Hi aj, I'm your DoorDash virtual assi...                     0   \n",
              "2  Chatbot: Hi Luke, I'm your DoorDash virtual as...                     0   \n",
              "3  Chatbot: Hi Colin, I'm your DoorDash virtual a...                     0   \n",
              "4  Chatbot: Hi Cassandra, I'm your DoorDash virtu...                     0   \n",
              "\n",
              "   ACTUAL_AC_CONVERSATION Parsed_AC CONVERSATION_HUMAN_AGENT  \n",
              "0                       0       0.0                        1  \n",
              "1                       0       0.0                        1  \n",
              "2                       0       0.0                        1  \n",
              "3                       0       0.0                        1  \n",
              "4                       0       0.0                        1  \n",
              "\n",
              "[5 rows x 195 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_dataset(path):\n",
        "    \"\"\"\n",
        "    Load dataset from CSV file with appropriate dtype handling.\n",
        "    \n",
        "    Args:\n",
        "        path: Path to the CSV file\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Loaded dataset\n",
        "    \"\"\"\n",
        "    print(f\"Loading dataset from: {path}\")\n",
        "    df = pd.read_csv(path, low_memory=False)\n",
        "    \n",
        "    # Convert DELIVERY_ID to float64 for consistency\n",
        "    if 'DELIVERY_ID' in df.columns:\n",
        "        df['DELIVERY_ID'] = df['DELIVERY_ID'].astype(np.float64)\n",
        "    \n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {len(df.columns)}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load the dataset\n",
        "df = load_dataset(DATASET_PATH)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "delivery_ids = df['DELIVERY_ID'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATASET INFO\n",
            "============================================================\n",
            "Total rows: 1,414,041\n",
            "Total columns: 195\n",
            "\n",
            "Target column distribution (IS_ESCALATED):\n",
            "IS_ESCALATED\n",
            "0    867967\n",
            "1    546074\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Escalation rate: 38.62%\n"
          ]
        }
      ],
      "source": [
        "# Display basic dataset info\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET INFO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total rows: {len(df):,}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "print(f\"\\nTarget column distribution ({TARGET_COLUMN}):\")\n",
        "if TARGET_COLUMN in df.columns:\n",
        "    print(df[TARGET_COLUMN].value_counts())\n",
        "    print(f\"\\nEscalation rate: {df[TARGET_COLUMN].mean():.2%}\")\n",
        "else:\n",
        "    print(f\"Warning: Target column '{TARGET_COLUMN}' not found in dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created defect category features:\n",
            "  - IS_ND (Never Delivered): 39,357 cases\n",
            "  - IS_MnI (Missing/Incorrect): 78,908 cases\n",
            "  - IS_PFQ (Quality Issue): 51,102 cases\n",
            "  - IS_OSI (Status Inquiry): 155,767 cases\n",
            "  - IS_LATE (Late/Early): 23,334 cases\n",
            "  - IS_WOD (Wrong Order): 13,120 cases\n"
          ]
        }
      ],
      "source": [
        "def create_defect_category_features(df):\n",
        "    \"\"\"\n",
        "    Create binary indicator features from DEFECT_CATEGORY.\n",
        "    \n",
        "    Features created:\n",
        "    - IS_ND: Never Delivered\n",
        "    - IS_MnI: Missing or Incorrect Items\n",
        "    - IS_PFQ: Order Quality Issue (Poor Food Quality)\n",
        "    - IS_OSI: Order Status Inquiry\n",
        "    - IS_LATE: Delivery Too Late / Early\n",
        "    - IS_WOD: Wrong Order Received\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with new features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # IS_ND: Never Delivered\n",
        "    df['IS_ND'] = (df['DEFECT_CATEGORY'] == 'Never Delivered').astype(int)\n",
        "    \n",
        "    # IS_MnI: Missing or Incorrect Items\n",
        "    df['IS_MnI'] = (df['DEFECT_CATEGORY'] == 'Missing or Incorrect Items').astype(int)\n",
        "    \n",
        "    # IS_PFQ: Order Quality Issue (Poor Food Quality)\n",
        "    df['IS_PFQ'] = (df['DEFECT_CATEGORY'] == 'Order Quality Issue').astype(int)\n",
        "    \n",
        "    # IS_OSI: Order Status Inquiry\n",
        "    df['IS_OSI'] = (df['DEFECT_CATEGORY'] == 'Order Status Inquiry').astype(int)\n",
        "    \n",
        "    # IS_LATE: Delivery Too Late / Early\n",
        "    df['IS_LATE'] = (df['DEFECT_CATEGORY'] == 'Delivery Too Late / Early').astype(int)\n",
        "    \n",
        "    # IS_WOD: Wrong Order Received\n",
        "    df['IS_WOD'] = (df['DEFECT_CATEGORY'] == 'Wrong Order Received').astype(int)\n",
        "    \n",
        "    print(\"Created defect category features:\")\n",
        "    print(f\"  - IS_ND (Never Delivered): {df['IS_ND'].sum():,} cases\")\n",
        "    print(f\"  - IS_MnI (Missing/Incorrect): {df['IS_MnI'].sum():,} cases\")\n",
        "    print(f\"  - IS_PFQ (Quality Issue): {df['IS_PFQ'].sum():,} cases\")\n",
        "    print(f\"  - IS_OSI (Status Inquiry): {df['IS_OSI'].sum():,} cases\")\n",
        "    print(f\"  - IS_LATE (Late/Early): {df['IS_LATE'].sum():,} cases\")\n",
        "    print(f\"  - IS_WOD (Wrong Order): {df['IS_WOD'].sum():,} cases\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Apply feature engineering\n",
        "df = create_defect_category_features(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Handling missing values for 70 features...\n",
            "  SH_IS_CREDITS: 1,204,982 nulls (85.2%) -> filled with 0\n",
            "  SH_IS_REFUND: 1,204,982 nulls (85.2%) -> filled with 0\n",
            "  SH_IS_REDELIVERY: 1,204,982 nulls (85.2%) -> filled with 0\n",
            "  SH_FIRST_REPORT_ISSUE: 1,204,982 nulls (85.2%) -> filled with 'Unknown'\n",
            "  SH_LATEST_REPORT_ISSUE: 1,204,982 nulls (85.2%) -> filled with 'Unknown'\n",
            "  MTO_ORDER_COUNT_L90D: 476,493 nulls (33.7%) -> filled with 0\n",
            "  FRAUD_CNR_REQUEST_RATIO_L60D: 10,609 nulls (0.8%) -> filled with 0\n",
            "  MTO_ORDER_COUNT_L12M: 476,493 nulls (33.7%) -> filled with 0\n",
            "  SH_IS_REJET: 1,204,982 nulls (85.2%) -> filled with 0\n",
            "  FRAUD_CNR_APPROVED_REQUESTS_COUNT_L60D: 9,625 nulls (0.7%) -> filled with 0\n",
            "  AVG_VP_LIFETIME: 5,248 nulls (0.4%) -> filled with 0\n",
            "  MTO_ORDER_COUNT_L28D: 476,493 nulls (33.7%) -> filled with 0\n",
            "  FRAUD_CNR_AMOUNT_L60D: 60,099 nulls (4.3%) -> filled with 0\n",
            "  ACTUAL_VP_RAW_AMT_L12M: 7,455 nulls (0.5%) -> filled with 0\n",
            "  MTO_ORDER_COUNT_LIFETIME: 476,493 nulls (33.7%) -> filled with 0\n",
            "  CREDIT_REFUND_ORDER_COUNT_L90D: 222,073 nulls (15.7%) -> filled with 0\n",
            "  MOST_FREQ_MTO_COUNT: 476,493 nulls (33.7%) -> filled with 0\n",
            "  FRAUD_CNR_REQUEST_RATIO_L180D: 9,625 nulls (0.7%) -> filled with 0\n",
            "  NEVER_DELIVERED_COUNT_L90D: 34,695 nulls (2.5%) -> filled with 0\n",
            "  DEFECT_DELIVERY_COUNT_L12M: 34,695 nulls (2.5%) -> filled with 0\n",
            "  HIGH_QUALITY_DELIVERY_COUNT_L12M: 34,695 nulls (2.5%) -> filled with 0\n",
            "  NEVER_DELIVERED_COUNT_L12M: 34,695 nulls (2.5%) -> filled with 0\n",
            "  ORDER_COUNT_L12M: 5,248 nulls (0.4%) -> filled with 0\n",
            "  ACTUAL_DELIVERIES_COUNT_L12M: 7,455 nulls (0.5%) -> filled with 0\n",
            "  FRAUD_CNR_AMOUNT_L180D: 60,099 nulls (4.3%) -> filled with 0\n",
            "  AVG_VP_LIFETIME_CATEGORY: 5,248 nulls (0.4%) -> filled with 'Unknown'\n",
            "  ORDER_COUNT_L90D: 5,248 nulls (0.4%) -> filled with 0\n",
            "  MOST_FREQ_MTO_ISSUE: 476,826 nulls (33.7%) -> filled with 'Unknown'\n",
            "  FRAUD_CNR_REQUESTED_DELIVERIES_COUNT_L180D: 9,625 nulls (0.7%) -> filled with 0\n",
            "  MTO_ORDER_COUNT_L7D: 476,493 nulls (33.7%) -> filled with 0\n",
            "  FRAUD_CNR_APPROVED_REQUESTS_COUNT_L180D: 9,625 nulls (0.7%) -> filled with 0\n",
            "  CREDIT_REFUND_ORDER_COUNT_L12M: 222,073 nulls (15.7%) -> filled with 0\n",
            "  ORDER_COUNT_LIFETIME: 5,248 nulls (0.4%) -> filled with 0\n",
            "  ML_CX_CNR_RISK_V1_SCORE: 55,692 nulls (3.9%) -> filled with 0\n",
            "  LATEST_MTO_ISSUE: 477,290 nulls (33.8%) -> filled with 'Unknown'\n",
            "  SUBTOTAL: 491,314 nulls (34.7%) -> filled with 0\n",
            "  HIGH_QUALITY_DELIVERY_COUNT_L90D: 34,695 nulls (2.5%) -> filled with 0\n",
            "  NEVER_DELIVERED_COUNT_L28D: 34,695 nulls (2.5%) -> filled with 0\n",
            "  TOTAL_ITEM_COUNT: 491,314 nulls (34.7%) -> filled with 0\n",
            "  IS_TOP_95_PERCENT_VP: 511 nulls (0.0%) -> filled with 'Unknown'\n",
            "  FRAUD_CNR_ND_ORDERS_COUNT_L60D: 38,359 nulls (2.7%) -> filled with 0\n",
            "  NEVER_DELIVERED_COUNT_LIFETIME: 34,695 nulls (2.5%) -> filled with 0\n",
            "  ORDER_COUNT_L28D: 5,248 nulls (0.4%) -> filled with 0\n",
            "  PROMOTIONS: 491,314 nulls (34.7%) -> filled with 0\n",
            "  AVG_SPEND_LIFETIME: 5,248 nulls (0.4%) -> filled with 0\n",
            "  DEFECT_DELIVERY_COUNT_L90D: 34,695 nulls (2.5%) -> filled with 0\n",
            "  DEFAULT_ZIP_CODE: 18,683 nulls (1.3%) -> filled with 'Unknown'\n",
            "  CREDIT_REFUND_ORDER_COUNT_L28D: 222,073 nulls (15.7%) -> filled with 0\n",
            "  AVG_GOV_LIFETIME: 5,248 nulls (0.4%) -> filled with 0\n",
            "  CREDIT_REFUND_ORDER_COUNT_LIFETIME: 222,073 nulls (15.7%) -> filled with 0\n",
            "  TOTAL_MAIN_VISITOR_COUNT_L90D: 1,623 nulls (0.1%) -> filled with 0\n",
            "  NEVER_DELIVERED_COUNT_L7D: 34,695 nulls (2.5%) -> filled with 0\n",
            "  FRAUD_CNR_ISSUANCE_AMOUNT_LIFETIME: 251,158 nulls (17.8%) -> filled with 0\n",
            "  DEFECT_DELIVERY_COUNT_LIFETIME: 34,695 nulls (2.5%) -> filled with 0\n",
            "  HIGH_QUALITY_DELIVERY_COUNT_L28D: 34,695 nulls (2.5%) -> filled with 0\n",
            "  FRAUD_CNR_GOV_AMOUNT_LIFETIME: 60,132 nulls (4.3%) -> filled with 0\n",
            "  SUBMIT_PLATFORM: 491,314 nulls (34.7%) -> filled with 'Unknown'\n",
            "  AVG_SPEND_LIFETIME_CATEGORY: 5,248 nulls (0.4%) -> filled with 'Unknown'\n",
            "  HOMEPAGE_SESSION_COUNT_L90D: 10,762 nulls (0.8%) -> filled with 0\n",
            "  EARLY_MORNING_COUNT_RATIO_LIFETIME: 5,248 nulls (0.4%) -> filled with 0\n",
            "  CANCEL_COUNT_LIFETIME: 34,695 nulls (2.5%) -> filled with 0\n",
            "  HIGH_QUALITY_DELIVERY_COUNT_LIFETIME: 34,695 nulls (2.5%) -> filled with 0\n"
          ]
        }
      ],
      "source": [
        "def handle_missing_values(df, features):\n",
        "    \"\"\"\n",
        "    Handle missing values for the specified features.\n",
        "    \n",
        "    Strategy:\n",
        "    - Numeric features: Fill with 0 (or median if preferred)\n",
        "    - Categorical features: Fill with 'Unknown' or mode\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        features: List of feature columns to process\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with handled missing values\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Check for missing features\n",
        "    missing_features = [f for f in features if f not in df.columns]\n",
        "    if missing_features:\n",
        "        print(f\"Warning: The following features are missing from the dataset:\")\n",
        "        for f in missing_features:\n",
        "            print(f\"  - {f}\")\n",
        "    \n",
        "    # Get available features\n",
        "    available_features = [f for f in features if f in df.columns]\n",
        "    \n",
        "    print(f\"\\nHandling missing values for {len(available_features)} features...\")\n",
        "    \n",
        "    null_filled_count = 0\n",
        "    for col in available_features:\n",
        "        null_count = df[col].isnull().sum()\n",
        "        if null_count > 0:\n",
        "            null_pct = null_count / len(df) * 100\n",
        "            null_filled_count += 1\n",
        "            \n",
        "            if col in CATEGORICAL_FEATURES:\n",
        "                # For categorical: fill with 'Unknown'\n",
        "                df[col] = df[col].fillna('Unknown')\n",
        "                print(f\"  {col}: {null_count:,} nulls ({null_pct:.1f}%) -> filled with 'Unknown'\")\n",
        "            else:\n",
        "                # For numeric: fill with 0\n",
        "                df[col] = df[col].fillna(0)\n",
        "                print(f\"  {col}: {null_count:,} nulls ({null_pct:.1f}%) -> filled with 0\")\n",
        "    \n",
        "    if null_filled_count == 0:\n",
        "        print(\"  No missing values found in the feature columns!\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Handle missing values\n",
        "df = handle_missing_values(df, FINAL_FEATURES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Type Conversions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 10 categorical features to 'str' dtype\n",
            "Converted 60 numeric features to 'float64' dtype\n"
          ]
        }
      ],
      "source": [
        "def convert_feature_dtypes(df, categorical_features, numeric_features):\n",
        "    \"\"\"\n",
        "    Convert features to appropriate data types for LightGBM.\n",
        "    \n",
        "    NOTE: For LightGBM Booster objects (not LGBMClassifier), we need to be careful\n",
        "    with categorical features. The categories must match exactly what was seen \n",
        "    during training. To avoid category mismatch errors, we keep categorical \n",
        "    features as strings and let LightGBM handle them during prediction.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        categorical_features: List of categorical feature names\n",
        "        numeric_features: List of numeric feature names\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with converted dtypes\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Convert categorical features to string (NOT pandas category)\n",
        "    # This avoids \"train and valid dataset categorical_feature do not match\" error\n",
        "    # when using LightGBM Booster objects\n",
        "    available_cat = [f for f in categorical_features if f in df.columns]\n",
        "    for col in available_cat:\n",
        "        df[col] = df[col].astype(str)\n",
        "    print(f\"Converted {len(available_cat)} categorical features to 'str' dtype\")\n",
        "    \n",
        "    # Convert numeric features\n",
        "    available_num = [f for f in numeric_features if f in df.columns]\n",
        "    for col in available_num:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('float64')\n",
        "    print(f\"Converted {len(available_num)} numeric features to 'float64' dtype\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Convert data types\n",
        "df = convert_feature_dtypes(df, CATEGORICAL_FEATURES, NUMERIC_FEATURES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Log Transformations (For Skewed Features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identified 52 skewed features for log1p transformation\n",
            "Identified 5 features with negative values (sign-preserving log)\n"
          ]
        }
      ],
      "source": [
        "def identify_skewed_features(df, numeric_features, skew_threshold=1.0):\n",
        "    \"\"\"\n",
        "    Identify highly skewed numeric features.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        numeric_features: List of numeric feature names\n",
        "        skew_threshold: Threshold for considering a feature as skewed\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (skewed_cols, negative_cols)\n",
        "    \"\"\"\n",
        "    available_num = [f for f in numeric_features if f in df.columns]\n",
        "    \n",
        "    # Calculate skewness\n",
        "    skew_values = df[available_num].skew()\n",
        "    \n",
        "    # Identify skewed columns\n",
        "    skewed_cols = skew_values[skew_values > skew_threshold].index.tolist()\n",
        "    \n",
        "    # Identify columns with negative values (need special handling)\n",
        "    negative_cols = []\n",
        "    for col in skewed_cols:\n",
        "        if (df[col] < 0).any():\n",
        "            negative_cols.append(col)\n",
        "    \n",
        "    # Remove negative columns from skewed_cols for standard log1p\n",
        "    skewed_cols = [col for col in skewed_cols if col not in negative_cols]\n",
        "    \n",
        "    print(f\"Identified {len(skewed_cols)} skewed features for log1p transformation\")\n",
        "    print(f\"Identified {len(negative_cols)} features with negative values (sign-preserving log)\")\n",
        "    \n",
        "    return skewed_cols, negative_cols\n",
        "\n",
        "# Identify skewed features\n",
        "skewed_cols, negative_cols = identify_skewed_features(df, NUMERIC_FEATURES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying log transformations...\n",
            "  Applied log1p to 52 features\n",
            "  Applied sign-preserving log to 5 features\n"
          ]
        }
      ],
      "source": [
        "def apply_log_transformations(df, skewed_cols, negative_cols):\n",
        "    \"\"\"\n",
        "    Apply log transformations to skewed features.\n",
        "    \n",
        "    - For non-negative skewed columns: log1p transformation\n",
        "    - For columns with negative values: sign-preserving log transformation\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        skewed_cols: Columns for standard log1p\n",
        "        negative_cols: Columns needing sign-preserving transformation\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Transformed DataFrame\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    print(\"Applying log transformations...\")\n",
        "    \n",
        "    # Standard log1p for non-negative skewed columns\n",
        "    transformed_count = 0\n",
        "    for col in skewed_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = np.log1p(df[col].astype(float))\n",
        "            transformed_count += 1\n",
        "    print(f\"  Applied log1p to {transformed_count} features\")\n",
        "    \n",
        "    # Sign-preserving log for columns with negative values\n",
        "    sign_transformed_count = 0\n",
        "    for col in negative_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = np.sign(df[col]) * np.log1p(np.abs(df[col].astype(float)))\n",
        "            sign_transformed_count += 1\n",
        "    print(f\"  Applied sign-preserving log to {sign_transformed_count} features\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply log transformations\n",
        "df.to_csv('/Users/shekhar.tanwar/Documents/Projects/NegotiatonAgent/dataset/processed_dataset/dataset_non_log.csv')\n",
        "df_transformed = apply_log_transformations(df, skewed_cols, negative_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEATURE VALIDATION REPORT\n",
            "============================================================\n",
            "Required features: 70\n",
            "Available features: 70\n",
            "Missing features: 0\n",
            "\n",
            "✓ All required features are available!\n"
          ]
        }
      ],
      "source": [
        "def validate_features(df, required_features):\n",
        "    \"\"\"\n",
        "    Validate that all required features are present in the DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        required_features: List of required feature names\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (available_features, missing_features)\n",
        "    \"\"\"\n",
        "    available = [f for f in required_features if f in df.columns]\n",
        "    missing = [f for f in required_features if f not in df.columns]\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"FEATURE VALIDATION REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Required features: {len(required_features)}\")\n",
        "    print(f\"Available features: {len(available)}\")\n",
        "    print(f\"Missing features: {len(missing)}\")\n",
        "    \n",
        "    if missing:\n",
        "        print(f\"\\nMissing features:\")\n",
        "        for f in missing:\n",
        "            print(f\"  - {f}\")\n",
        "    else:\n",
        "        print(\"\\n✓ All required features are available!\")\n",
        "    \n",
        "    return available, missing\n",
        "\n",
        "# Validate features\n",
        "available_features, missing_features = validate_features(df_transformed, FINAL_FEATURES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Statistics (Sample of first 10 features):\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SH_CNR</th>\n",
              "      <th>SH_IS_CREDITS</th>\n",
              "      <th>SH_IS_REFUND</th>\n",
              "      <th>SH_IS_REDELIVERY</th>\n",
              "      <th>IS_MnI</th>\n",
              "      <th>MTO_ORDER_COUNT_L90D</th>\n",
              "      <th>FRAUD_CNR_REQUEST_RATIO_L60D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.414041e+06</td>\n",
              "      <td>1.414041e+06</td>\n",
              "      <td>1.414041e+06</td>\n",
              "      <td>1.414041e+06</td>\n",
              "      <td>1.414041e+06</td>\n",
              "      <td>1.414041e+06</td>\n",
              "      <td>1.414041e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-2.739978e-01</td>\n",
              "      <td>7.704495e-02</td>\n",
              "      <td>2.217811e-02</td>\n",
              "      <td>3.255344e-03</td>\n",
              "      <td>3.867982e-02</td>\n",
              "      <td>3.799907e-01</td>\n",
              "      <td>1.169246e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.048549e+00</td>\n",
              "      <td>2.178706e-01</td>\n",
              "      <td>1.219870e-01</td>\n",
              "      <td>4.739026e-02</td>\n",
              "      <td>1.591059e-01</td>\n",
              "      <td>7.019965e-01</td>\n",
              "      <td>1.422012e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-6.931472e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-6.931472e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-6.931472e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.550755e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-6.931472e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.931472e-01</td>\n",
              "      <td>1.670541e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.809883e+00</td>\n",
              "      <td>6.931472e-01</td>\n",
              "      <td>6.931472e-01</td>\n",
              "      <td>6.931472e-01</td>\n",
              "      <td>6.931472e-01</td>\n",
              "      <td>5.384495e+00</td>\n",
              "      <td>1.403994e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             SH_CNR  SH_IS_CREDITS  SH_IS_REFUND  SH_IS_REDELIVERY  \\\n",
              "count  1.414041e+06   1.414041e+06  1.414041e+06      1.414041e+06   \n",
              "mean  -2.739978e-01   7.704495e-02  2.217811e-02      3.255344e-03   \n",
              "std    1.048549e+00   2.178706e-01  1.219870e-01      4.739026e-02   \n",
              "min   -6.931472e-01   0.000000e+00  0.000000e+00      0.000000e+00   \n",
              "25%   -6.931472e-01   0.000000e+00  0.000000e+00      0.000000e+00   \n",
              "50%   -6.931472e-01   0.000000e+00  0.000000e+00      0.000000e+00   \n",
              "75%   -6.931472e-01   0.000000e+00  0.000000e+00      0.000000e+00   \n",
              "max    5.809883e+00   6.931472e-01  6.931472e-01      6.931472e-01   \n",
              "\n",
              "             IS_MnI  MTO_ORDER_COUNT_L90D  FRAUD_CNR_REQUEST_RATIO_L60D  \n",
              "count  1.414041e+06          1.414041e+06                  1.414041e+06  \n",
              "mean   3.867982e-02          3.799907e-01                  1.169246e-01  \n",
              "std    1.591059e-01          7.019965e-01                  1.422012e-01  \n",
              "min    0.000000e+00          0.000000e+00                  0.000000e+00  \n",
              "25%    0.000000e+00          0.000000e+00                  0.000000e+00  \n",
              "50%    0.000000e+00          0.000000e+00                  7.550755e-02  \n",
              "75%    0.000000e+00          6.931472e-01                  1.670541e-01  \n",
              "max    6.931472e-01          5.384495e+00                  1.403994e+00  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display feature statistics\n",
        "print(\"\\nFeature Statistics (Sample of first 10 features):\")\n",
        "print(\"=\" * 60)\n",
        "sample_features = available_features[:10]  # Show first 10 features\n",
        "df_transformed[sample_features].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Load Pre-trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: /Users/shekhar.tanwar/Documents/Projects/NegotiatonAgent/model/v1_lgb_1125_fold3.pkl\n",
            "Model loaded successfully!\n",
            "Model type: Booster\n"
          ]
        }
      ],
      "source": [
        "def load_model(model_path):\n",
        "    \"\"\"\n",
        "    Load pre-trained LightGBM model from pickle file.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to the pickle file\n",
        "        \n",
        "    Returns:\n",
        "        Loaded model object\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    \n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "    \n",
        "    with open(model_path, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    \n",
        "    print(f\"Model loaded successfully!\")\n",
        "    print(f\"Model type: {type(model).__name__}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Load the model\n",
        "model = load_model(MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Information:\n",
            "============================================================\n",
            "Model expects 48 features\n",
            "First 10 features: ['SH_LATEST_REPORT_ISSUE', 'TOTAL_ITEM_COUNT', 'SH_FIRST_REPORT_ISSUE', 'PAYMENT_METHOD', 'DEFECT_CATEGORY', 'SH_CNR', 'FRAUD_CNR_APPROVED_REQUESTS_COUNT_L60D', 'SH_IS_CREDITS', 'SUBTOTAL', 'TIP']\n"
          ]
        }
      ],
      "source": [
        "# Display model information\n",
        "print(\"\\nModel Information:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try to get model features (depends on model type)\n",
        "try:\n",
        "    if hasattr(model, 'feature_name_'):\n",
        "        model_features = model.feature_name_\n",
        "        print(f\"Model expects {len(model_features)} features\")\n",
        "        print(f\"First 10 features: {model_features[:10]}\")\n",
        "    elif hasattr(model, 'feature_name'):\n",
        "        model_features = model.feature_name()\n",
        "        print(f\"Model expects {len(model_features)} features\")\n",
        "        print(f\"First 10 features: {model_features[:10]}\")\n",
        "    else:\n",
        "        print(\"Could not retrieve model feature names directly\")\n",
        "        print(\"Will use FINAL_FEATURES configuration\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting model info: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MODEL INSPECTION\n",
            "============================================================\n",
            "\n",
            "Model was trained with 48 features\n",
            "\n",
            "Feature comparison:\n",
            "  - Features matching: 34\n",
            "  - In model but not in our list: 14\n",
            "  - In our list but not in model: 36\n",
            "\n",
            "Features in model but not in our FINAL_FEATURES:\n",
            "  - AVG_TIP_LIFETIME\n",
            "  - CREDIT_REFUND_ORDER_COST_LIFETIME\n",
            "  - FRAUD_CNR_REQUEST_RATIO_LD_L180D\n",
            "  - FREQ_CATEGORY\n",
            "  - IF_DASHER_MANUAL_ASSIGNED\n",
            "  - IS_LATE\n",
            "  - IS_NON_ABUSER\n",
            "  - IS_WHALE_CX\n",
            "  - ORDER_COUNT_L7D\n",
            "  - ORDER_TIME_OF_DAY\n",
            "  - PAYMENT_METHOD\n",
            "  - SH_CNR_RATIO\n",
            "  - TIP\n",
            "  - TIP_PERCENTAGE\n",
            "\n",
            "Features in our FINAL_FEATURES but not in model:\n",
            "  - ACTUAL_DELIVERIES_COUNT_L12M\n",
            "  - AVG_GOV_LIFETIME\n",
            "  - AVG_SPEND_LIFETIME_CATEGORY\n",
            "  - AVG_VP_LIFETIME\n",
            "  - AVG_VP_LIFETIME_CATEGORY\n",
            "  - CANCEL_COUNT_LIFETIME\n",
            "  - CREDIT_REFUND_ORDER_COUNT_L12M\n",
            "  - CREDIT_REFUND_ORDER_COUNT_L28D\n",
            "  - CREDIT_REFUND_ORDER_COUNT_L90D\n",
            "  - CREDIT_REFUND_ORDER_COUNT_LIFETIME\n",
            "  - DEFECT_DELIVERY_COUNT_L12M\n",
            "  - DEFECT_DELIVERY_COUNT_L90D\n",
            "  - DEFECT_DELIVERY_COUNT_LIFETIME\n",
            "  - EARLY_MORNING_COUNT_RATIO_LIFETIME\n",
            "  - FRAUD_CNR_AMOUNT_L180D\n",
            "  - FRAUD_CNR_AMOUNT_L60D\n",
            "  - FRAUD_CNR_GOV_AMOUNT_LIFETIME\n",
            "  - FRAUD_CNR_ISSUANCE_AMOUNT_LIFETIME\n",
            "  - HIGH_QUALITY_DELIVERY_COUNT_L12M\n",
            "  - HIGH_QUALITY_DELIVERY_COUNT_L28D\n",
            "  - HIGH_QUALITY_DELIVERY_COUNT_L90D\n",
            "  - HIGH_QUALITY_DELIVERY_COUNT_LIFETIME\n",
            "  - HOMEPAGE_SESSION_COUNT_L90D\n",
            "  - IS_TOP_95_PERCENT_VP\n",
            "  - MTO_ORDER_COUNT_L28D\n",
            "  - MTO_ORDER_COUNT_L7D\n",
            "  - NEVER_DELIVERED_COUNT_L12M\n",
            "  - NEVER_DELIVERED_COUNT_L28D\n",
            "  - NEVER_DELIVERED_COUNT_L7D\n",
            "  - NEVER_DELIVERED_COUNT_L90D\n",
            "  - NEVER_DELIVERED_COUNT_LIFETIME\n",
            "  - ORDER_COUNT_L12M\n",
            "  - ORDER_COUNT_L28D\n",
            "  - ORDER_COUNT_L90D\n",
            "  - ORDER_COUNT_LIFETIME\n",
            "  - TOTAL_MAIN_VISITOR_COUNT_L90D\n",
            "\n",
            "============================================================\n",
            "USING MODEL'S FEATURE LIST FOR PREDICTION\n",
            "============================================================\n",
            "Will use these 48 features in exact order from model\n"
          ]
        }
      ],
      "source": [
        "# Inspect the model to understand its structure\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL INSPECTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get feature names from the model\n",
        "model_feature_names = model.feature_name()\n",
        "print(f\"\\nModel was trained with {len(model_feature_names)} features\")\n",
        "\n",
        "# Check if our features match the model features\n",
        "model_feature_set = set(model_feature_names)\n",
        "our_feature_set = set(FINAL_FEATURES)\n",
        "\n",
        "matching = model_feature_set.intersection(our_feature_set)\n",
        "in_model_not_ours = model_feature_set - our_feature_set\n",
        "in_ours_not_model = our_feature_set - model_feature_set\n",
        "\n",
        "print(f\"\\nFeature comparison:\")\n",
        "print(f\"  - Features matching: {len(matching)}\")\n",
        "print(f\"  - In model but not in our list: {len(in_model_not_ours)}\")\n",
        "print(f\"  - In our list but not in model: {len(in_ours_not_model)}\")\n",
        "\n",
        "if in_model_not_ours:\n",
        "    print(f\"\\nFeatures in model but not in our FINAL_FEATURES:\")\n",
        "    for f in sorted(in_model_not_ours):\n",
        "        print(f\"  - {f}\")\n",
        "\n",
        "if in_ours_not_model:\n",
        "    print(f\"\\nFeatures in our FINAL_FEATURES but not in model:\")\n",
        "    for f in sorted(in_ours_not_model):\n",
        "        print(f\"  - {f}\")\n",
        "\n",
        "# IMPORTANT: Use the model's feature list for prediction\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"USING MODEL'S FEATURE LIST FOR PREDICTION\")\n",
        "print(\"=\"*60)\n",
        "available_features = model_feature_names\n",
        "print(f\"Will use these {len(available_features)} features in exact order from model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Generate Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Required features: 48\n",
            "Available features: 47\n",
            "\n",
            "WARNING: Missing 1 features!\n",
            "Missing features: ['SH_CNR_RATIO']\n",
            "\n",
            "Creating missing features with default values...\n",
            "\n",
            "Generating predictions for 1,414,041 samples...\n",
            "Using 48 features in model's expected order\n",
            "Encoded 7 categorical features as numeric\n",
            "\n",
            "✓ Predictions generated successfully!\n",
            "Prediction range: [0.0016, 0.8534]\n",
            "Mean prediction: 0.1323\n"
          ]
        }
      ],
      "source": [
        "def generate_predictions(model, df, features, categorical_features):\n",
        "    \"\"\"\n",
        "    Generate predictions using the trained LightGBM Booster model.\n",
        "    \n",
        "    The key to avoiding \"train and valid dataset categorical_feature do not match\" error\n",
        "    is to convert the DataFrame to a numpy array. LightGBM Booster objects store\n",
        "    categorical feature information from training, and when you pass a pandas DataFrame,\n",
        "    it checks if the categories match. By converting to numpy array, we bypass this check.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained LightGBM model (Booster object)\n",
        "        df: DataFrame with features\n",
        "        features: List of feature names to use (in exact order expected by model)\n",
        "        categorical_features: List of categorical feature names\n",
        "        \n",
        "    Returns:\n",
        "        np.array: Prediction probabilities\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    \n",
        "    # Check which features are available\n",
        "    available = [f for f in features if f in df.columns]\n",
        "    missing = [f for f in features if f not in df.columns]\n",
        "    \n",
        "    print(f\"Required features: {len(features)}\")\n",
        "    print(f\"Available features: {len(available)}\")\n",
        "    \n",
        "    if missing:\n",
        "        print(f\"\\nWARNING: Missing {len(missing)} features!\")\n",
        "        print(\"Missing features:\", missing)\n",
        "        print(\"\\nCreating missing features with default values...\")\n",
        "        for f in missing:\n",
        "            if f in categorical_features:\n",
        "                df[f] = 'Unknown'\n",
        "            else:\n",
        "                df[f] = 0.0\n",
        "        available = features  # Now all features should be available\n",
        "    \n",
        "    # Prepare feature matrix - IMPORTANT: maintain exact column order from model\n",
        "    X = df[features].copy()\n",
        "    \n",
        "    # Encode categorical features as numeric (required for numpy array approach)\n",
        "    label_encoders = {}\n",
        "    cat_cols_in_features = [c for c in features if c in categorical_features]\n",
        "    \n",
        "    for col in features:\n",
        "        if col in categorical_features:\n",
        "            le = LabelEncoder()\n",
        "            X[col] = X[col].fillna('Unknown').astype(str)\n",
        "            X[col] = le.fit_transform(X[col])\n",
        "            label_encoders[col] = le\n",
        "        else:\n",
        "            X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0).astype(np.float64)\n",
        "    \n",
        "    print(f\"\\nGenerating predictions for {len(X):,} samples...\")\n",
        "    print(f\"Using {len(features)} features in model's expected order\")\n",
        "    print(f\"Encoded {len(label_encoders)} categorical features as numeric\")\n",
        "    \n",
        "    # Convert to numpy array - this bypasses pandas categorical mismatch issues\n",
        "    X_array = X.values.astype(np.float64)\n",
        "    \n",
        "    # Generate predictions\n",
        "    pred_probs = model.predict(X_array)\n",
        "    \n",
        "    print(f\"\\n✓ Predictions generated successfully!\")\n",
        "    print(f\"Prediction range: [{pred_probs.min():.4f}, {pred_probs.max():.4f}]\")\n",
        "    print(f\"Mean prediction: {pred_probs.mean():.4f}\")\n",
        "    \n",
        "    return pred_probs\n",
        "\n",
        "# Generate predictions using the model's exact feature list\n",
        "predictions = generate_predictions(model, df_transformed, available_features, CATEGORICAL_FEATURES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prediction Distribution:\n",
            "PREDICTED_ESCALATION\n",
            "0    1400910\n",
            "1      13131\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Predicted escalation rate: 0.93%\n"
          ]
        }
      ],
      "source": [
        "# Add predictions to DataFrame\n",
        "df_transformed['PREDICTED_ESCALATION_PROB'] = predictions\n",
        "df_transformed['PREDICTED_ESCALATION'] = (predictions >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\nPrediction Distribution:\")\n",
        "print(df_transformed['PREDICTED_ESCALATION'].value_counts())\n",
        "print(f\"\\nPredicted escalation rate: {df_transformed['PREDICTED_ESCALATION'].mean():.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_transformed['DELIVERY_ID'] = delivery_ids\n",
        "df_transformed[['DELIVERY_ID']+ FINAL_FEATURES + ['PREDICTED_ESCALATION_PROB','PREDICTED_ESCALATION']].to_csv('/Users/shekhar.tanwar/Documents/Projects/NegotiatonAgent/dataset/processed_dataset/dataset_predictions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>DELIVERY_ID</th>\n",
              "      <th>CX_ID</th>\n",
              "      <th>STORE_ID</th>\n",
              "      <th>DEFECT_DATE</th>\n",
              "      <th>DEFECT_CATEGORY</th>\n",
              "      <th>DEFECT_TIMESTAMP_UTC</th>\n",
              "      <th>IS_DP_CX</th>\n",
              "      <th>IS_ELITE_CX</th>\n",
              "      <th>IS_WHALE_CX</th>\n",
              "      <th>...</th>\n",
              "      <th>CONVERSATION_HUMAN_AGENT</th>\n",
              "      <th>IS_ND</th>\n",
              "      <th>IS_MnI</th>\n",
              "      <th>IS_PFQ</th>\n",
              "      <th>IS_OSI</th>\n",
              "      <th>IS_LATE</th>\n",
              "      <th>IS_WOD</th>\n",
              "      <th>SH_CNR_RATIO</th>\n",
              "      <th>PREDICTED_ESCALATION_PROB</th>\n",
              "      <th>PREDICTED_ESCALATION</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2.179857e+11</td>\n",
              "      <td>1949223925</td>\n",
              "      <td>29736287</td>\n",
              "      <td>2025-09-05</td>\n",
              "      <td>Unknown or Unspecified Issue</td>\n",
              "      <td>2025-09-05 18:10:40.536000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102361</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>3.560231e+11</td>\n",
              "      <td>1884080059</td>\n",
              "      <td>1130627</td>\n",
              "      <td>2025-09-03</td>\n",
              "      <td>Unknown or Unspecified Issue</td>\n",
              "      <td>2025-09-03 17:18:12.930000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008399</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>2.621826e+11</td>\n",
              "      <td>76381204</td>\n",
              "      <td>1130627</td>\n",
              "      <td>2025-09-03</td>\n",
              "      <td>Missing or Incorrect Items</td>\n",
              "      <td>2025-09-03 23:13:03.000000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.349462</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12</td>\n",
              "      <td>2.768723e+11</td>\n",
              "      <td>258188252</td>\n",
              "      <td>1130627</td>\n",
              "      <td>2025-09-03</td>\n",
              "      <td>Unknown or Unspecified Issue</td>\n",
              "      <td>2025-09-04 01:42:29.205000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.198509</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>3.184242e+11</td>\n",
              "      <td>1965318695</td>\n",
              "      <td>281439</td>\n",
              "      <td>2025-09-06</td>\n",
              "      <td>Unknown or Unspecified Issue</td>\n",
              "      <td>2025-09-06 17:12:46.960000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.185310</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 204 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0   DELIVERY_ID       CX_ID  STORE_ID DEFECT_DATE  \\\n",
              "0           0  2.179857e+11  1949223925  29736287  2025-09-05   \n",
              "1           2  3.560231e+11  1884080059   1130627  2025-09-03   \n",
              "2          11  2.621826e+11    76381204   1130627  2025-09-03   \n",
              "3          12  2.768723e+11   258188252   1130627  2025-09-03   \n",
              "4          43  3.184242e+11  1965318695    281439  2025-09-06   \n",
              "\n",
              "                DEFECT_CATEGORY           DEFECT_TIMESTAMP_UTC  IS_DP_CX  \\\n",
              "0  Unknown or Unspecified Issue  2025-09-05 18:10:40.536000000         1   \n",
              "1  Unknown or Unspecified Issue  2025-09-03 17:18:12.930000000         0   \n",
              "2    Missing or Incorrect Items  2025-09-03 23:13:03.000000000         1   \n",
              "3  Unknown or Unspecified Issue  2025-09-04 01:42:29.205000000         0   \n",
              "4  Unknown or Unspecified Issue  2025-09-06 17:12:46.960000000         1   \n",
              "\n",
              "   IS_ELITE_CX  IS_WHALE_CX  ...  CONVERSATION_HUMAN_AGENT  IS_ND    IS_MnI  \\\n",
              "0          0.0            0  ...                         1    0.0  0.000000   \n",
              "1          0.0            0  ...                         1    0.0  0.000000   \n",
              "2          0.0            0  ...                         1    0.0  0.693147   \n",
              "3          0.0            0  ...                         1    0.0  0.000000   \n",
              "4          0.0            0  ...                         1    0.0  0.000000   \n",
              "\n",
              "  IS_PFQ IS_OSI  IS_LATE  IS_WOD  SH_CNR_RATIO PREDICTED_ESCALATION_PROB  \\\n",
              "0    0.0    0.0        0       0           0.0                  0.102361   \n",
              "1    0.0    0.0        0       0           0.0                  0.008399   \n",
              "2    0.0    0.0        0       0           0.0                  0.349462   \n",
              "3    0.0    0.0        0       0           0.0                  0.198509   \n",
              "4    0.0    0.0        0       0           0.0                  0.185310   \n",
              "\n",
              "  PREDICTED_ESCALATION  \n",
              "0                    0  \n",
              "1                    0  \n",
              "2                    0  \n",
              "3                    0  \n",
              "4                    0  \n",
              "\n",
              "[5 rows x 204 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_transformed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df_transformed[['DELIVERY_ID','PREDICTED_ESCALATION']].to_csv('/Users/shekhar.tanwar/Documents/Projects/NegotiatonAgent/dataset/processed_dataset/dataset_predictions.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Evaluation (If Labels Available)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(df, target_col, pred_prob_col):\n",
        "    \"\"\"\n",
        "    Evaluate model performance if true labels are available.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with predictions and labels\n",
        "        target_col: Name of the target column\n",
        "        pred_prob_col: Name of the prediction probability column\n",
        "        \n",
        "    Returns:\n",
        "        dict: Performance metrics\n",
        "    \"\"\"\n",
        "    if target_col not in df.columns:\n",
        "        print(f\"Target column '{target_col}' not found. Skipping evaluation.\")\n",
        "        return None\n",
        "    \n",
        "    # Get valid samples (non-null labels)\n",
        "    valid_mask = df[target_col].notna()\n",
        "    y_true = df.loc[valid_mask, target_col].astype(int)\n",
        "    y_pred_prob = df.loc[valid_mask, pred_prob_col]\n",
        "    \n",
        "    print(f\"Evaluating on {len(y_true):,} samples with valid labels\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # ROC AUC\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    \n",
        "    # PR AUC\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "    \n",
        "    # Metrics at different thresholds\n",
        "    print(\"\\nPrecision & Recall at different thresholds:\")\n",
        "    threshold_list = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    results = []\n",
        "    \n",
        "    for th in threshold_list:\n",
        "        y_pred = (y_pred_prob >= th).astype(int)\n",
        "        tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
        "        fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
        "        fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
        "        \n",
        "        precision_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        \n",
        "        results.append({\n",
        "            'threshold': th,\n",
        "            'precision': round(precision_val, 4),\n",
        "            'recall': round(recall_val, 4)\n",
        "        })\n",
        "    \n",
        "    metrics_df = pd.DataFrame(results)\n",
        "    print(metrics_df.to_string(index=False))\n",
        "    \n",
        "    return {\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'threshold_metrics': results\n",
        "    }\n",
        "\n",
        "# Evaluate model\n",
        "metrics = evaluate_model(df_transformed, TARGET_COLUMN, 'PREDICTED_ESCALATION_PROB')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC and PR curves\n",
        "if TARGET_COLUMN in df_transformed.columns:\n",
        "    valid_mask = df_transformed[TARGET_COLUMN].notna()\n",
        "    y_true = df_transformed.loc[valid_mask, TARGET_COLUMN].astype(int)\n",
        "    y_pred_prob = df_transformed.loc[valid_mask, 'PREDICTED_ESCALATION_PROB']\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
        "    \n",
        "    axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
        "    axes[0].plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random')\n",
        "    axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "    axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "    axes[0].set_title('ROC Curve', fontsize=14)\n",
        "    axes[0].legend(loc='lower right')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # PR Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    \n",
        "    axes[1].plot(recall, precision, 'b-', linewidth=2, label=f'PR (AUC = {pr_auc:.3f})')\n",
        "    axes[1].set_xlabel('Recall', fontsize=12)\n",
        "    axes[1].set_ylabel('Precision', fontsize=12)\n",
        "    axes[1].set_title('Precision-Recall Curve', fontsize=14)\n",
        "    axes[1].legend(loc='lower left')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Target column not available. Skipping curve plots.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot prediction probability distribution\n",
        "if TARGET_COLUMN in df_transformed.columns:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    valid_mask = df_transformed[TARGET_COLUMN].notna()\n",
        "    df_valid = df_transformed[valid_mask]\n",
        "    \n",
        "    # Histogram by class\n",
        "    for label, color in [(0, 'blue'), (1, 'red')]:\n",
        "        subset = df_valid[df_valid[TARGET_COLUMN] == label]['PREDICTED_ESCALATION_PROB']\n",
        "        axes[0].hist(subset, bins=50, alpha=0.6, color=color, \n",
        "                     label=f'Class {label}', density=True)\n",
        "    \n",
        "    axes[0].set_xlabel('Predicted Probability', fontsize=12)\n",
        "    axes[0].set_ylabel('Density', fontsize=12)\n",
        "    axes[0].set_title('Prediction Probability Distribution', fontsize=14)\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Box plot by class\n",
        "    df_valid.boxplot(column='PREDICTED_ESCALATION_PROB', by=TARGET_COLUMN, ax=axes[1])\n",
        "    axes[1].set_xlabel('Actual Label', fontsize=12)\n",
        "    axes[1].set_ylabel('Predicted Probability', fontsize=12)\n",
        "    axes[1].set_title('Prediction Distribution by Class', fontsize=14)\n",
        "    plt.suptitle('')  # Remove automatic title\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    # Just show overall distribution\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(df_transformed['PREDICTED_ESCALATION_PROB'], bins=50, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Predicted Probability', fontsize=12)\n",
        "    plt.ylabel('Count', fontsize=12)\n",
        "    plt.title('Prediction Probability Distribution', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final results summary\n",
        "print(\"=\" * 60)\n",
        "print(\"ESCALATION MODEL PIPELINE - RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nDataset: {DATASET_PATH}\")\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"\\nTotal samples processed: {len(df_transformed):,}\")\n",
        "print(f\"Features used: {len(available_features)}\")\n",
        "\n",
        "print(f\"\\nPrediction Statistics:\")\n",
        "print(f\"  - Mean probability: {df_transformed['PREDICTED_ESCALATION_PROB'].mean():.4f}\")\n",
        "print(f\"  - Median probability: {df_transformed['PREDICTED_ESCALATION_PROB'].median():.4f}\")\n",
        "print(f\"  - Std deviation: {df_transformed['PREDICTED_ESCALATION_PROB'].std():.4f}\")\n",
        "print(f\"  - Min: {df_transformed['PREDICTED_ESCALATION_PROB'].min():.4f}\")\n",
        "print(f\"  - Max: {df_transformed['PREDICTED_ESCALATION_PROB'].max():.4f}\")\n",
        "\n",
        "print(f\"\\nPredicted Escalations (threshold=0.5):\")\n",
        "print(f\"  - Escalated: {(df_transformed['PREDICTED_ESCALATION'] == 1).sum():,}\")\n",
        "print(f\"  - Not Escalated: {(df_transformed['PREDICTED_ESCALATION'] == 0).sum():,}\")\n",
        "print(f\"  - Predicted escalation rate: {df_transformed['PREDICTED_ESCALATION'].mean():.2%}\")\n",
        "\n",
        "if metrics:\n",
        "    print(f\"\\nModel Performance:\")\n",
        "    print(f\"  - ROC AUC: {metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  - PR AUC: {metrics['pr_auc']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "display_cols = ['DELIVERY_ID', 'DEFECT_CATEGORY', 'PREDICTED_ESCALATION_PROB', 'PREDICTED_ESCALATION']\n",
        "if TARGET_COLUMN in df_transformed.columns:\n",
        "    display_cols.append(TARGET_COLUMN)\n",
        "\n",
        "available_display = [c for c in display_cols if c in df_transformed.columns]\n",
        "df_transformed[available_display].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Export Results (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to export predictions to CSV\n",
        "# output_path = '/Users/shekhar.tanwar/Documents/Projects/NegotiatonAgent/dataset/predictions/escalation_predictions.csv'\n",
        "# \n",
        "# export_cols = ['DELIVERY_ID', 'DEFECT_CATEGORY', 'PREDICTED_ESCALATION_PROB', 'PREDICTED_ESCALATION']\n",
        "# if TARGET_COLUMN in df_transformed.columns:\n",
        "#     export_cols.append(TARGET_COLUMN)\n",
        "# \n",
        "# # Create output directory if it doesn't exist\n",
        "# os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "# \n",
        "# df_transformed[export_cols].to_csv(output_path, index=False)\n",
        "# print(f\"Predictions exported to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pipeline Complete!\n",
        "\n",
        "This notebook has:\n",
        "1. ✅ Loaded the dataset from CSV\n",
        "2. ✅ Created derived features (IS_ND, IS_MnI, IS_PFQ, IS_OSI, etc.)\n",
        "3. ✅ Handled missing values\n",
        "4. ✅ Converted data types for LightGBM\n",
        "5. ✅ Applied log transformations to skewed features\n",
        "6. ✅ Loaded the pre-trained model (v1_lgb_1125_fold3.pkl)\n",
        "7. ✅ Generated escalation predictions\n",
        "8. ✅ Evaluated model performance (if labels available)\n",
        "9. ✅ Visualized results\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
